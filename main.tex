\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[bulgarian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Записки по случайни процеси и Марковски вериги}
\author{Мирослав Стояновски}
\date{\today}

\begin{document}

\maketitle

\section{Въведение}
\subsection{Марковски вериги - въведение}
Колекция от случайни величини X = (Xn)n\geq0 се нарича случаен процес в дискретно време.

\section{Дефиниции}
\subsection{Дефиниция за дискретна Марковска верига}
1. P (X0 = i) = $\lambda_i$, i$\in$I \\
2. В сила е Марковското свойство за всяка траектория $\{i_0,...,i_n\}$ с полож. вероятност \\
P(X$_{n+1}$ = j | X$_n$ = i; $\cdots$ ; X$_0$ = i$_0$) = P(X$_{n+1}$ = j | X$_n$ = i) =: p$_{i,j}$

\subsection{Нехомогенна Марковска верига}
Нехомог. МВ: Матрицата може да е различна за различно n.

\section{Модел на Еренфест}
\subsection{Анализ на модела}
Защо $f(n) = |E[X_n] - N/2|$ намалява с $n$ в модела на Еренфест? \\
$E[X_{n+1} | X_n=M] = (M-1)\cdot\frac{M}{N} + (M+1)\cdot\frac{N-M}{N} = M\left(1-\frac{2}{N}\right)+1$ или $E[X_{n+1}|X_n]=X_n\left(1-\frac{2}{N}\right)+1$.
Но $E[E[X_{n+1}|X_n]]=E[X_n]\left(1-\frac{2}{N}\right)+1$, от друга страна пък $E[E[X_{n+1}|X_n]]=E[X_{n+1}]$ или $E[X_{n+1}]=E[X_n]\left(1-\frac{2}{N}\right)+1$.
После изразяваме $E[X_n]$ със $E[X_0]$ и сравняваме $f(n+1)$ и $f(n)$, откъдето виждаме, че $f(n+1)<f(n)$ за вс. $n$.

\section{Предварителни свойства}
\subsection{Характеризация на Марковски вериги}
X = (X$_n$)$_{n\geq0}$ е МВ $\Leftrightarrow$ P(X$_0$=i$_0$,$\cdots$,X$_n$=i$_n$)=$\lambda_{i_0}\times p_{i_0i_1}\times\cdots\times p_{i_{n-1}i_n}$ за всяка траектория

\subsubsection*{Бонус:}
X = (X$_n$)$_{n\geq0}$ е МВ $\Leftrightarrow$ P(X$_{n+1}$=i$_{n+1}$,...,X$_{n+m}$=i$_{n+m}$|X$_n$=i$_n$)=$p_{i_ni_{n+1}},...,p_{i_{n+m-1}i_{n+m}}$ за всеки n,m, траект.

\subsection{Доказателство за характеризация на МВ}
$\Rightarrow$) индукция + усл. вероятност. \\
$\Leftarrow$) Взимаме произв. траектория с дължина k+1, разбиваме на усл. вер. за посл. състояние, съкращаваме
траекторията до k от двете страни, и накрая поучаваме $p_{i_ki_{k+1}}$ - готови сме.

\subsection{Следствие}
За X = (X$_n$)$_{n\geq0}$ е МВ, разпределението (P(X$_n$ = i))$_{i\in I}$ се задава чрез $\lambda P^n$. \\
Д-во: индукция - разделяме P(X$_1$=j) на сума от усл. вероятности(ф-ла за пълна вер.) спрямо X$_0$ и получаваме, че 
P(X$_1$=j) е j-тата колонка на $\lambda P$. Разделяме P(X$_{n+1}$=j) по същия начин спрямо X$_n$ - j-та колонка на $\lambda P^n$.

\subsection{Чапман-Колмогоров}
За всяка МВ $(p_{ij})^{(n+m)}=\sum_{k\in I} (p_{ik})^m(p_{kj})^n$

\subsection{Диагонализация на матрици}
Намираме собствени ст-ти чрез det(P-$\lambda$E)=0. Намираме съотв. собств. в-ри чрез тези собств. ст-ти.: Px=$\lambda$x.
Ако собств. ст-ти са разл., то матрицата е диагонализируема.
P=SDS$^{-1}$, където S е получена от собств. в-ри наредени по колонки, D е със собст. ст-ти по главния диаг. и другите нули. 
S$^{-1}$ се намира като (S|E) се сведе до (E|S$^{-1}$) чрез елементарни матрични преобразования. 
Също така P$^n$=SD$^n$S$^{-1}$. За неразл., положително възвратни, апериодични МВ limP$^n$ се схожда до матрица I$\times$I, където всеки ред е коорд. на стац. в-р. 
Скоростта на сходимост зависи от втората по големина собствена стойност - тя е по-малка от 1. 
Ако I е крайно, то 1 винаги е собств. ст-т. \\
Полезен трик, който може да се използва при намирането на $p_{ij}^{(n)}$ е че то е линейна комбинация на собствените стойности $\mu_i$, тоест $\sum c_i\mu_i$ за някои $c_i\in \mathbb{R}$. Константите могат да се намерят от първите няколко степени на матрицата. Това обикновено е по-бърз метод за намиране на единични стойности на $P^n$, защото спестява търсенето на собствените вектори и на $S^{-1}$.

\section{Времена на достигане}
\subsection{Дефиниции и основни свойства}
$H^A = \min\{n \geq 0 : X_n \in A\}$

\subsubsection*{Лема:}
$P_\lambda(H^A < \infty) = \sum_{j=0}^\infty P_\lambda(X_0 \notin A, X_1 \notin A, \dots, X_{j-1} \notin A, X_j \in A)$

\subsection{Пример с баскетболист}
Търсим $P_0(H^{\{N\}} < \infty)$ за произв. N. \\
Взимаме $U_0 = 0$, $U_1 = \min\{n > U_0 : X_n = 0\}$, \dots, $U_{k+1} = \min\{n > U_k : X_n = 0\}$. 
Тогава $\{H^{\{N\}} = \infty\} = \bigcap_{k=0}^\infty \{U_{k+1} - U_k \leq N\}$. 
Когато смятаме вероятността на това събитие, то се разбива на произв. на геометрични, защото 
$U_{k+1} - U_k$ са i.i.d. $\mathrm{Ge}(q)$ за всяко $k$, а $q = 1 - p$ е вероятността баскетболиста да не уцели.

\subsection{Леми за времена на достигане}
\subsubsection*{Лема 1:}
Ако $i \notin A$, то за всяко $m \geq 1$ и $j \in I$: \\
$P_i(H^A = m, X_1 = j) = p_{ij} \cdot P_j(H^A = m-1)$

\subsubsection*{Лема 2:}
Ако $i \notin A$, то за всяко $j \in I$: \\
$P_i(H^A < \infty, X_1 = j) = p_{ij} \cdot P_j(H^A < \infty)$

\subsubsection*{Доказателство:}
Тъй като $i \notin A$, преиндексираме и изкарваме +1 отпред. Разбиваме на условна вероятност и прилагаме просто Марковско свойство. За безкрайното, сумираме по $m$.

\subsection{Означения и теореми}
\subsubsection*{Означения:}
$h_i^A = P_i(H^A < \infty) \in [0, 1]$, \quad $k_i^A = E_i[H^A] \in [0, \infty]$

\subsubsection*{Теорема за $h^A$:}
Нека $h^A = (h_i^A)_{i \in I}$. Тогава $h^A$ е минималното решение на системата:
\begin{align*}
h_i^A &= 1 \quad \text{за } i \in A, \\
h_i^A &= \sum_{j \in I} p_{ij} h_j^A \quad \text{иначе}
\end{align*}

\subsubsection*{Забележка:}
$h^A = P^A h^A$, където в $P^A$ редовете с индекс $i \in A$ са заменени с $\delta_i$.

\subsubsection*{Доказателство:}
$h_i^A = \sum_{j \in I} P_i(H^A < \infty, X_1 = j)$ и прилагаме горната лема - доказваме че е решение. 
За минималност, взимаме $v$ - друго решение на системата. 
Развиваме $v_i$ по втората релация и разбиваме на $X_1 \in A$ и $X_1 \notin A$. 
Повтаряме $l$ пъти и махаме положителния остатък с неравенство: $v_i \geq \sum_{n=1}^l P_i(H^A = n)$. 
При $l \to \infty$ получаваме $v_i \geq h_i^A$.

\subsection{Диференчни уравнения}
$pf(i+1) - f(i) + qf(i-1) = 0$, $f(0) = 1$ - хомогенно диференчно уравнение. \\
Решаваме характеристичния полином $px^2 - x + q = 0$: $x_1 = 1$, $x_2 = q/p$. \\
Ако корените са различни: $f(i) = A x_1^i + B x_2^i$. \\
Ако корените са равни: $f(i) = A x_1^i + B i x_1^i$. \\

За нехомогенни уравнения:
\begin{enumerate}
\item Решаваме хомогенното
\item Търсим конкретен корен на нехомогенното (пробваме $C$, $Ci$, $Ci^2$, ...)
\item Намираме константата $C$
\item Общо решение: конкретно решение нехомогенно + общо решение хомогенно
\end{enumerate}

Начални условия и граници определят константите $A,B,C,...$ в зависимост от $p,q$.

\subsubsection*{Полезна формула:}
За неотрицателни сл. вел., $E[X] = \sum_{m \geq 1} P(X \geq m)$

\subsubsection*{Теорема за $k^A$:}
Нека $k^A = (k_i^A)_{i \in I}$. Тогава $k^A$ е минималното решение на системата:
\begin{align*}
k_i^A &= 0 \quad \text{за } i \in A, \\
k_i^A &= 1 + \sum_{j \in I} p_{ij} k_j^A \quad \text{иначе}
\end{align*}

\subsubsection*{Забележки:}
Сумата може да е по $I$ или по $A^c$ без значение тъй като $k_j^A=0$ за елементи от А. Минималното решение може да е $\infty$.

\subsubsection*{Доказателство:}
$k_i^A = E_i[H^A] = \sum_{m \geq 1} P_i(H^A \geq m)$ \\
Разбиваме по $X_1$, прилагаме условна вероятност, преиндексираме, Марковско свойство. 
Отделяме $m=1$ (дава 1), останалото става търсена сума. 
Доказателство за минималност е аналогично на това за $h_i^A$.

\section{Комуникация и свързаност}
\subsection{Дефиниции}
\subsubsection*{Дефиниция:}
$i \to j$, ако $P_i(H^{\{j\}} < \infty) = P_i\left(\bigcup_{n=0}^\infty \{X_n = j\}\right) > 0$

\subsubsection*{Теорема:}
$i \to j \Leftrightarrow p_{ij}^{(n)} > 0$ за някое $n$. Също така $\leftrightarrow$ е релация на еквивалентност. \\
$\Rightarrow)$ Допускане на противното. \\
$\Leftarrow)$ От дефиниция на $i \to j$. \\
За релация на еквивалентност: рефлексивност и симетричност са тривиални. Транзитивност се доказва чрез Чапман-Колмогоров.

\subsection{Класове на комуникация}
\subsubsection*{Дефиниция:}
$\leftrightarrow$ разделя $I$ на класове на еквивалентност $C_i = \{j \in I : i \leftrightarrow j\}$ - класове на комуникация. \\
Класът $C$ е \textbf{затворен} по дефиниция ако $i \in C$, $i \to j \Rightarrow j \in C$.

\subsubsection*{Дефиниция:}
МВ е \textbf{неразложима} ако съществува само един (затворен) клас на комуникация.

\section{Просто Марковско Свойство}
\subsection{Дефиниции}
\subsubsection*{Условна независимост}
Нека $P(C) > 0$. Казваме, че събитията $A$ и $B$ са условно независими при условие $C$, ако:
$$P(A \cap B | C) = P(A | C)P(B | C)$$

\subsubsection*{Просто Марковско Свойство}
Нека $X = (X_n)_{n\geq0}$ е Марковска верига, $i \in I$, $P(X_m = i) > 0$. Тогава, при условие че $\{X_m = i\}$:
\begin{enumerate}
\item $Y = (Y_n)_{n\geq0} := (X_{n+m})_{n\geq0}$ е $(\delta_i, P)$ Марковска верига
\item $Y$ е независима от $(X_0, \dots, X_m)$
\end{enumerate}

\subsection{Доказателство}
\subsubsection*{Доказателство за 1.}
Взимаме произволна траектория в бъдещето:
$$A = \{X_m = i, X_{m+1} = i_1, \dots, X_{m+N} = i_N\}$$

$P(A)$ представяме като сумираме по всички състояния преди $m$ и заместваме от теоремата за характеризация:
$$P(A) = \sum_{j_0,\dots,j_{m-1}} P(X_0 = j_0, \dots, X_{m-1} = j_{m-1}, X_m = i, \dots, X_{m+N} = i_N)$$

Изкарваме нещата след $m$ от сумата:
$$= p_{i,i_1}\cdots p_{i_{N-1},i_N} \sum_{j_0,\dots,j_{m-1}} P(X_0 = j_0, \dots, X_{m} = i)$$

Сумата става $P(X_m = i)$ и делим двете страни на $P(X_m = i)$:
$$P(A | X_m = i) = p_{i,i_1}\cdots p_{i_{N-1},i_N}$$

Което показва, че $Y$ има същото вероятностно разпределение като $X$, но започващо от $\delta_i$.

\subsubsection*{Доказателство за 2.}
Взимаме произволна траектория в миналото:
$$B = \{X_0 = j_0, X_1 = j_1, \dots, X_m = j_m\}$$

Търсим да докажем, че:
$$P(A \cap B | X_m = i) = P(A | X_m = i)P(B | X_m = i)$$

При $j_m\neq i$ е тривиално. Нека $j_m=i$. Тогава:
\begin{align*}
P(A \cap B | \{X_m = i\}) &= \frac{P(A \cap B \cap \{X_m = i\})}{P(X_m = i)} \\
&= \frac{P(A | B \cap \{X_m = i\})P(B \cap \{X_m = i\})}{P(X_m = i)} \\
&= P(A | \{X_m = i\})P(B | \{X_m = i\})
\end{align*}

Където за последното равенство сме използвали че \\ $P(A | B\cap \{X_m = i\}) = P(A|\{X_m=i\})$ от дефиницията на МВ.
Това завършва доказателството.

\section{Изчезване на разклоняващ се процес - бонус материал}
\subsection{Основни дефиниции и свойства}
Означаваме с $h_\lambda = P_\lambda(H^{\{0\}} < \infty)$ - вероятността за изчезване на епидемията. \\ 
Нека $p_{1j} = P(\xi = j) =: q_j$ е вероятността един да зарази $j$.

\subsection{Уравнение за вероятността на изчезване}

Ако $X_0 = i$, може да допуснем, че имаме $i$ на брой независими Марковски вериги с един начален болен. 
За изчезване на всички болни, трябва всички $i$ вериги да достигнат 0, т.е. $h_i = h_1^i$. \\

Тогава от системата, $h_1$ удовлетворява:
$$ h_1 = \sum_{j=0}^\infty p_{1j} h_j = \sum_{j=0}^\infty p_{1j} h_1^j = \sum_{j=0}^\infty q_j h_1^j = \mathbb{E}[h_1^\xi]= f_\xi(h_1)$$

Което е пораждащата функция на $\xi$.

\subsection{Метод за решаване}
\begin{enumerate}
\item В примерна задача е дадено разпределението на $\xi$
\item Намираме $f_\xi$ по дефиниция
\item Решаваме уравнението $h_1 = f_\xi(h_1)$
\item Намираме $h_1$, което е единственото минимално решение
\end{enumerate}

\subsection{Стохастично начало}
За стохастично начало, например $X_0 \sim \lambda_p$ (вместо фиксирано $i$ както досега), изчисляваме:
$$ h_{\lambda_p} = \sum_{j=0}^\infty P(X_0 = j) h_j $$

\section{Силно Марковско Свойство}
\subsection{Дефиниции}
\subsubsection*{Дефиниция за случайно време}
Случайно време е случайна величина $T: \Omega \to \mathbb{N} \cup \{\infty\}$

\subsubsection*{Дефиниция за Марковски момент}
$T$ е \textbf{Марковски момент}, ако $\{T=n\} \in \sigma(X_0,\dots,X_n)$ за всяко $n$

\subsubsection*{Дефиниция за консистентно събитие}
$B \subseteq \Omega$ е \textbf{консистентно} спрямо Марковски момент $T$, ако \\
$B \cap \{T=n\} \in \sigma(X_0,X_1,\dots,X_n)$ за всяко $n$

\subsection{Силно Марковско Свойство (СМС)}
За всяка Марковска верига, за всеки Марковски момент $T$, всяко $i \in I$, всяко $\{T<\infty, X_T=i\}$ с $P>0$ е изпълнено:
\begin{enumerate}
\item $(X_{n+T})_{n\geq0}$ и $(X_0,\dots,X_T)$ са условно независими спрямо $\{T<\infty, X_T=i\}$
\item При условие $\{T<\infty, X_T=i\}$, $(X_{n+T})_{n\geq0}$ е $(\delta_i,P)$ Марковска верига
\end{enumerate}

\subsubsection*{Еквивалентна формулировка}
Условия 1 и 2 са еквивалентни на: За всяко $B$ консистентно спрямо $T$, $n\geq1$ и траектория:
$$P_\lambda(X_{T+1}=j_1,\dots,X_{T+n}=j_n,B|T<\infty,X_T=i) = P_i(X_1=j_1,\dots,X_n=j_n)P_\lambda(B|T<\infty,X_T=i)$$

\subsection{Доказателство (бонус)}
Идеята е да докажем горното равенство, от което следват условия 1 и 2. \\

Тъй като $B$ представлява миналото на веригата, равенството ни казва, че за всяка траектория в бъдещето $X_{T+1},\dots,X_{T+n}$, тя е независима от миналото $B$. Тъй като $B$ е произволно, то равенството е вярно и за $B=\Omega$, тоест не се интересуваме
какво е миналото, включваме всички траектории до момента $X_T$. Идеята на това $B$ е да работим само със консистентни множества, вместо да разписваме всички траектории до момента $T$ по характеризацията. Те се включват в B, защото то е произволно. Това ще улесни доказателството.\\

Нека вземем произволен Марковски момент $T$, $i\in I$, $\{T<\infty, X_T=i\}$ с положителна вероятност, $n\geq1$, $B$ - консистентно спрямо $T$. \\

Нека $C=\{X_T=i,X_{T+1}=l_1,\dots,X_{T+n}=l_n\}$ е произволна траектория. Тогава $P_\lambda(B,T<\infty,C)$ представлява цялата траектория от началото до бъдещето, ще го сведем до
горното равенство:
\begin{align*}
P_\lambda(B,T<\infty,C) &= \sum_{m=0}^\infty P_\lambda(B,T=m,X_m=i,X_{m+1}=l_1,\dots,X_{m+n}=l_n) \\
&= \sum_{m=0}^\infty P_\lambda(B,T=m, X_m=i)P_\lambda(X_{m+1}=l_1,\dots,X_{m+n}=l_n|B, T=m,X_m=i) \\
&= P_i(X_1=l_1,\dots,X_n=l_n)P_\lambda(B,T<\infty,X_T=i)
\end{align*}

Разделяйки двете страни на $P_\lambda(T<\infty,X_T=i)$, получаваме търсеното равенство.

\subsection{Приложения}
\subsubsection*{Означения}
\begin{align*}
T_1^i &:= \min\{n > H^i : X_n = i\}, \\
&\vdots \\
T_{k+1}^i &:= \min\{n > T_k^i : X_n = i\}, \\
U_1 &:= T_1^i - H^i, \\
&\vdots \\
U_{k+1} &:= T_{k+1}^i - T_k^i
\end{align*}

Ако $P_j(H^i<\infty)=1$ за произволно $j$(вкл. и $i$), то от СМС:
\begin{itemize}
\item $P_i(U_1<\infty)=1$
\item $U_k$ са i.i.d.
\item $P_j(T_{k+1}^i<\infty) = P_j\left(H^i + \sum_{l=1}^{k+1} U_l < \infty\right) = P_j(H^i<\infty)P_i(U_1<\infty)^{k+1} = 1$
\end{itemize}

\subsubsection*{Брой завръщания като мярка за важност}
Нека $X$ е неразложима Марковска верига, $P(X_0=j)=1$, $P_j(H^i<\infty)=1$. \\

Дефинираме мярка за важност:
$$v_i = \lim_{N\to\infty} \frac{N_i}{N} \in [0,1]$$
където $N_i$ е брой посещения на $i$ до момента $N$. \\

Имаме:
\begin{align*}
T_{N_i}^i &> N \geq T_{N_i-1}^i \\
\lim_{N\to\infty} \frac{T_{N_i}^i}{N_i} &= \lim_{N\to\infty} \frac{H^i + \sum_{m=1}^{N_i} U_m}{N_i} \overset{\text{п.с.}}{=} E_i[U_1]
\end{align*}

От $N\to\infty \Rightarrow N_i\to\infty$ и теоремата за полицаите получаваме:
$$\frac{N}{N_i} \to E_i[U_1] \quad \Rightarrow \quad v_i = \frac{1}{E_i[U_1]}$$

\subsubsection*{Бонус:}
Ако МВ има и стационарен вектор, то е вярно:
\begin{itemize}
\item $\sum_{i\in I} v_i = 1$
\item $v_i = \pi_i$ (стационарно разпределение)
\end{itemize}

\section{Възвратност и преходност на МВ}

\subsection{Означения}
\begin{itemize}
\item $\{A_n \text{ б.ч.}\} = \bigcap_{n>0} \bigcup_{k\geq n} A_k = \left\{\sum_{k=0}^\infty 1_{A_k} = \infty\right\}$
\item $A_i := \{X_n = i \text{ б.ч.}\}$
\item $V_i := \min\{m \geq 1 : T_m^i = \infty\} \in [1,\infty]$
\end{itemize}

\subsection{Лема}
За МВ е вярно че:
\begin{enumerate}
\item $P_i(T_m < \infty) = P_i(T_1 < \infty)^m$ \quad (1)
\item $V_i \sim \mathrm{Ge}(P_i(T_1 = \infty))$ \quad (2)
\item $P_i(V_i = \infty) \in \{0,1\}$, като $P_i(V_i = \infty) = 0 \Leftrightarrow P_i(T_1 < \infty) < 1$ \quad (3)
\end{enumerate}

\subsubsection*{Доказателство:}
(1): $P_i(T_m < \infty) = P_i(T_{m-1} < \infty; \min\{n > T_{m-1} : X_n = i\} < \infty)$ - разбиваме на условна и прилагаме СМС. \\
Тогава се получава $P_i(T_m < \infty) = P_i(T_1 < \infty)P_i(T_{m-1} < \infty)$ и после рекурсивно. \\

(2): $P_i(V_i > m) = P_i(T_m < \infty) = P_i(T_1 < \infty)^m$ - това е дефиницията на $\mathrm{Ge}(P_i(T_1 = \infty))$ \\

(3): $P_i(V_i = \infty) = \lim_{m \to \infty} P_i(V_i > m) = \lim_{m \to \infty} P_i(T_1 < \infty)^m$. \\
От тази граница виждаме че за $m \to \infty$, ако $P_i(T_1 < \infty) = 1$, то $P_i(V_i = \infty) = 1$. Ако $P_i(T_1 < \infty) < 1$, то $P_i(V_i = \infty) = 0$.

\subsection{Забележки}
\begin{itemize}
\item $P_i(V_i = m) = P_i\left(\sum_{k=0}^\infty 1_{\{X_k = i\}} = m\right)$
\item $P_i(V_i = \infty) = P_i(A_i)$
\item Следователно $P_i(A_i) \in \{0,1\}$
\end{itemize}

\subsection{Дефиниции}
\begin{itemize}
\item $i \in I$ е \textbf{възвратно} $\Leftrightarrow P_i(A_i) = 1 \Leftrightarrow P_i(T_1 < \infty) = 1$
\item $i \in I$ е \textbf{преходно} $\Leftrightarrow P_i(A_i) = 0 \Leftrightarrow P_i(T_1 < \infty) < 1$
\end{itemize}

\subsection{Теорема за критерии за възвратност}
$i \in I$ е възвратно $\Leftrightarrow E_i[V_i] = \infty \Leftrightarrow \sum_{n=0}^\infty p_{ii}^{(n)} = \infty \Leftrightarrow \sum_{n=0}^\infty P_i(X_n = i) = \infty$

\subsubsection*{Доказателство на първото $\Leftrightarrow$:}
$\Rightarrow$) $i \in I$ е възвратно $\Rightarrow P_i(V_i = \infty) = 1 \Rightarrow E_i[V_i] = \infty$ \\

$\Leftarrow$) Допускаме противното - $i$ е преходно, т.е. $P_i(V_i = \infty) = 0$ \\
Но $V_i \sim \mathrm{Ge}(P_i(T_1 = \infty))$, тоест $E_i[V_i] = \frac{1}{p} = \frac{1}{P_i(T_1 = \infty)}$ \\
Но $P_i(V_i = \infty) = 0$ и от лема (3) $P_i(T_1 < \infty) < 1$, т.е. $P_i(T_1 = \infty) > 0$. \\
Тогава $E_i[V_i] = \frac{1}{P_i(T_1 = \infty)} < \infty$. Противоречие.

\subsubsection*{Доказателство на второто $\Leftrightarrow$:}
$E_i[V_i] = E_i\left[\sum_{k=0}^\infty 1_{\{X_k = i\}}\right] = \cdots = \sum_{k=0}^\infty p_{ii}^{(k)}$

\subsection{Теорема за класове на комуникация}
В един клас на комуникация $C$ всички състояния са или възвратни, или преходни. (не е нужно МВ да е крайна)

\subsubsection*{Доказателво:}
Нека $i \in C$ е преходно. Нека $j \in C$ е друго състояние (ако в класа има само $i$, то тв. е тривиално). 
Тогава $i \leftrightarrow j$, или $p_{ij}^{(l_1)}p_{ji}^{(l_2)} > 0$. $i$ е преходно, т.е. $\sum_{k=0}^\infty p_{ii}^{(k)}$ е крайна. 
Чрез преиндексиране и използване на теоремата на Чапман-Колмогоров показваме че \\
$\sum_{k=0}^\infty p_{jj}^{(k)} \leq \sum_{k=0}^\infty p_{ii}^{(k)} < \infty$. \\ Това от горната теорема значи 
че $j$ е преходно. Но $j$ беше произволно от $C$. Тоест всички състояния в $C$ са преходни. \\
Доказахме че ако има 1 преходно, всички са преходни. Следователно за да съществува поне 
едно възвратно в $C$, ще трябва всички да са възвратни, иначе от току що доказаното 
ако има преходно, всички са преходни.

\subsection{Забележка}
Свойствата възвратност и преходност не зависят от началното състояние. 
Те са свойства само при започване от даденото състояние. Възможно е едно състояние $i$ да 
е възвратно обаче никога да не бъде посетено от една траектория, 
например защото тази траектория ще си е почнала в някакъв затворен клас на комуникация, 
а пък $i$ да не принадлежи в този клас.

\subsection*{Домашна работа}
Да се докаже че модела на Еренфест е възвратна МВ чрез $\sum_{k=0}^\infty p_{00}^{(k)}$

\subsection{Теорема за връзка между възвратност и затвореност}
\begin{enumerate}
\item Възвратен КК $\Rightarrow$ Затворен КК \quad (1)
\item Краен и затворен КК $\Rightarrow$ Възвратен КК \quad (2)
\end{enumerate}

\subsubsection*{Доказателство (1):}
Допускаме противното, т.е. имаме следното инфо: \\
$P_i\left(\left(\sum_{n=0}^\infty 1_{\{X_n = i\}}\right) = \infty\right) = 1$, $P_i(H^j < \infty) > 0$, $P_j(H^i < \infty) = 0$ \\
Тогава $0 < P_i(H^j < \infty) = P_i\left(H^j < \infty; \left(\sum_{n=0}^\infty 1_{\{X_n = i\}}\right) = \infty\right)$. Второто може да го добавим, 
защото има вероятност 1. Тогава преиндексираме сумата да започва от $n = H^j$ нататък. 
Това може да го направим, защото във горната вероятност имаме $H^j < \infty$. Тъй като $H^j$ е крайно, 
то сумирането, започващо от $H^j$ нататък вместо от 0 също е безкрайно и вероятността не се променя. 
Тогава от СМС, това е нова верига, започваща от $j$ и независима от $X_0,...,X_{H^j}$. Тоест, може да разбием сумата така: \\
$P_i\left(H^j < \infty; \left(\sum_{n=0}^\infty 1_{\{X_n = i\}}\right) = \infty\right) = P_i(H^j < \infty)P_j\left(\sum_{n=0}^\infty 1_{\{Y_n = i\}}\right) = \infty) \leq P_i(H^j < \infty)P_j(H^i < \infty) = 0$. Противоречие.

\subsubsection*{Доказателство (2):}
Идеята е да вземем някое $i \in C$ от което да започва веригата и да докажем че произв. $j \in C$ е възвратно. \\
Ясно е, че $\infty = \sum_{n=0}^\infty 1_{\{X_n \in C\}} = \sum_{n=0}^\infty \sum_{j \in C} 1_{\{X_n = j\}}$. Тъй като $C$ е крайно 
множество, то може да разменим сумите и получаваме, че \\ $P_i\left(\sum_{j \in C} \sum_{n=0}^\infty 1_{\{X_n = j\}} = \infty\right) = 1$. \\
$|C| < \infty$ и следователно има такова $j$, което прави сумата безкрайна, или $P_i\left(\sum_{n=0}^\infty 1_{\{X_n = j\}} = \infty\right) = 1$.
Вижда се, че \\ $P_i\left(H^j < \infty \mid \sum_{n=0}^\infty 1_{\{X_n = j\}} = \infty\right) = 1$, тоест \\ $P_i\left(H^j < \infty \cap \sum_{n=0}^\infty 1_{\{X_n = j\}} = \infty\right) = 1$. \\
Чрез подобно преиндексиране и СМС като отгоре, получаваме \\ $P_i(H^j < \infty)P_j\left(\sum_{n=0}^\infty 1_{\{X_n = j\}} = \infty\right) = 1$. \\
Тъй като и двата члена са в интервала $[0;1]$, то щом произведението им е 1, то и поотделно са $= 1$. \\
Тоест $P_j\left(\sum_{n=0}^\infty 1_{\{X_n = j\}} = \infty\right) = 1$, или $P_j(X_n = j \text{ б.ч.}) = 1$, което по дефиниция значи, че $j$ възвратно.

\subsection{Теорема за неразложима и възвратна МВ}
Неразложима и възвратна МВ $\Rightarrow P_\lambda(T_1^j < \infty) = 1$

\subsubsection*{Доказателство:}
$P_\lambda(T_1^j < \infty) = \sum_{i \in I} \lambda_i P_i(T_1^j < \infty)$. Идеята е да докажем, че $P_i(T_1^j < \infty) = 1$, откъдето сумираме 
само по $\lambda_i$, което е 1. \\ $P_i(T_1^j < \infty) = P_i(H^j < \infty)P_j(T_1^j < \infty)$ от СМС. Но МВ е възвратна, тоест $P_j(T_1^j < \infty) = 1$. 
Трябва да докажем $P_i(H^j < \infty) = 1$. Първо трябва да докажем, че $P_i(H^j < T_1^i) > 0$, защото иначе 
няма да можем да представим $H^j$ като момент между $T_{m-1}^i$ и $T_m^i$ за някое $m$, което ще ни потрябва 
след малко в доказателството. Нека засега допуснем, че сме доказали $P_i(H^j < T_1^i) > 0$. \\
Нека $U = \min\{m \geq 1 : H^j \in (T_{m-1}^i, T_m^i)\}$. Чрез усл. вероятност, СМС и рекурсия виждаме, че 
$P_i(U > k) = (1 - P_i(H^j < T_1^i))^k$. Това в скобите е $<1$ и за голямо $k$, вероятността клони към 0, тоест $P_i(U < \infty) = 1$. 
Това ще каже че винаги има такова $m$, такова че $H^j < T_m^i$, започвайки от $i$. Също така, $i$ е възвратно 
и има безкрайно много завръщания. От тези двете неща, виждаме че $P_i(H^j < \infty) = 1$. Това търсихме да докажем. 
Но защо $P_i(H^j < T_1^i) > 0$? От възвратност на $i$ и от неразложимост ($P_i(H^j < \infty) > 0$), следва че вероятността 
$H^j$ да е между $T_{m-1}^i$ и $T_m^i$ за някое $m$ е ненулева, тъй като има безкрайно много такива моменти $T_m^i$.  
Взимаме най-малкото такова $m$. Но ако разгледаме МВ, започваща от момента $T_{m-1}^i$, то от СМС следва, 
че вероятността $H^j$ да е преди $T_1^i$ е ненулева, тоест $P_i(H^j < T_1^i) > 0$, което е търсеното.

\section{Възвратност или преходност на случайно блуждаене - бонус материал}

\subsection{1D случайно блуждаене}
Искаме да проверим дали $\sum_{n=0}^\infty p_{00}^{(n)} = \infty$. Може и $p_{00}^{(2n)}$, защото сме 
в 0 само за четен бр. стъпки. $p_{00}^{(2n)} = \binom{2n}{n}\frac{1}{2^{2n}} = \frac{1}{2^{2n}}\frac{(2n)!}{(n!)^2}$. \\

Приближението на Стирлинг казва че $n! \sim \sqrt{2\pi n}n^n e^{-n}$ при $n \to \infty$. \\

След заместване горе и съкратяване получаваме че $p_{00}^{(2n)} \sim \frac{1}{\sqrt{2\pi n}}$. \\

Сумата не схожда, защото порядъка е $\frac{1}{\sqrt{n}}$. Следователно 0 е възвратно.

\subsection{2D случайно блуждаене}
Искаме да проверим дали:
$$\sum_{n=0}^\infty P_{(0,0)}(S_n = (0,0)) = \sum_{n=0}^\infty P_{(0,0)}(S_{2n} = (0,0)) = \infty$$

Може със Стирлинг и комбинаторика, подобно на това горе. Може и с ротации, както ще обясним сега. \\

Ако $X_j = (V_j, W_j)$, то $S_n = \left(\sum_{j=1}^n V_j, \sum_{j=1}^n W_j\right) = (S_n^{(1)}, S_n^{(2)})$. \\

Търсим дали $\sum_{n=0}^\infty P_{(0,0)}(S_{2n}^{(1)} = 0, S_{2n}^{(2)} = 0) = \infty$. Не може да разбием сумата, \\
тъй като $S_{2n}^{(1)}$ и $S_{2n}^{(2)}$ не са независими. \\

Нека дефинираме ротация:
\begin{align*}
X_j = (1,0) &\Leftrightarrow Y_j = (1,1) \\
X_j = (0,1) &\Leftrightarrow Y_j = (-1,1) \\
X_j = (-1,0) &\Leftrightarrow Y_j = (-1,-1) \\
X_j = (0,-1) &\Leftrightarrow Y_j = (1,-1)
\end{align*}

Нека $Y_j = (U_j, H_j)$. Проверяваме, че $U_j$ и $H_j$ са независими по дефиниция на независимост. \\

Ако $S_{2n} = (0,0)$, то:
\begin{align*}
\sum_{j=1}^{2n} V_j = 0 \hspace{0.5em} \land \hspace{0.5em} \sum_{j=1}^{2n} W_j = 0 \quad &\Leftrightarrow \quad \sum_{j=1}^{2n} U_j = 0  \hspace{0.5em} \land \hspace{0.5em}
\sum_{j=1}^{2n} H_j = 0
\end{align*}

Тоест търсената сума се свежда от суми на $V$ и $W$ до суми на $U$ и $H$. Тъй като $U$ и $H$ са независими, то \\
вероятността на сумите по $U$ и $H$ се разбива на произведение на вероятности:
$$P\left(\sum_{j=1}^{2n} U_j = 0, \sum_{j=1}^{2n} H_j = 0\right) = P\left(\sum_{j=1}^{2n} U_j = 0\right) \cdot P\left(\sum_{j=1}^{2n} H_j = 0\right)$$

Всяка от тези двете е все едно отделна независима случайна разходка в 1D. От 1D случая, знаем че \\
порядъка на 1D случая е $\frac{1}{\sqrt{n}}$. Тук се умножават и порядъка е $\frac{1}{n}$, което отново не схожда. \\

Следователно и в 2D случая 0 е възвратно.

\section{Стационарно разпределение и стационарна мярка}

\subsection{Дефиниции}
\subsubsection*{Стационарно разпределение/мярка}
$\lambda$ е стационарно разпределение (мярка), ако $\lambda = \lambda P$. Стационарното разпределение обикновено се означава с $\pi$.

\subsection{Теореми}
\subsubsection*{Теорема за инвариантност}
Ако $\lambda$ е стационарен вектор, то $P_\lambda(X_n = i) = \lambda_i$, $\forall i \in I$, $\forall n \in \mathbb{N}$.

\subsubsection*{Доказателство:}
Знаем, че $(P_\lambda(X_n = i))_{i \in I} = \lambda P^n$. Но $\lambda$ е стационарен вектор и $\lambda P^n = \lambda$. Тогава $(P_\lambda(X_n = i))_{i \in I} = \lambda$.

\subsubsection*{Теорема за гранично поведение}
\begin{enumerate}
\item Нека имаме МВ с $|I| < \infty$ и вземем някое $i \in I$. Ако $\lim_{n \to \infty} p_{ij}^{(n)} \to \pi_j$ за всяко $j \in I$, то $\pi$ е стационарен вектор.
\item Нека имаме МВ с $|I| = \infty$ и вземем някое $i \in I$. Ако $\lim_{n \to \infty} p_{ij}^{(n)} \to \pi_j$ за всяко $j \in I$ и $\pi$ се сумира до 1, то $\pi$ е стационарен вектор.
\end{enumerate}

\subsubsection*{Наблюдение:}
С времето няма значение от кое състояние сме започнали, границата клони към $\pi_j$, което не зависи от $i$.

\subsubsection*{Доказателво (1):}
Имаме $p_{ij}^{(n)} = \sum_{k \in I} p_{ik}^{(n-1)}p_{kj}$ от уравнението на Чапман-Колмогоров. Взимаме границата от двете страни. \\
Границата може да влезе в сумата, защото сумата е крайна. Накрая получаваме $\pi_j = \sum_{k \in I} \pi_k p_{kj}$. Това от дясно \\
е $j$-тата колонка на $\pi P$, тоест $\pi = \pi P$ и $\pi$ е стационарен вектор.

\subsection{Означения и теореми за $\gamma_i^k$}
\subsubsection*{Означения:}
$\gamma_i^k := E_k\left[\sum_{n=0}^{T_1^k - 1} 1_{\{X_n = i\}}\right]$, $\gamma^k := (\gamma_i^k)_{i \in I}$

\subsubsection*{Наблюдения:}
\begin{itemize}
\item При $i \neq k$ може сумата да се преиндексира от $n=1$ до $T_1^k$
\item Ако $k$ е възвратно (или $T_1^k < \infty$), то може и при $i=k$
\item Полезен формат: $\gamma_i^k = \sum_{n=1}^\infty P_k(X_n = i; n \leq T_1^k)$
\item $\sum_{i \in I} \gamma_i^k = E_k[T_1^k] =: m_k$ - по дефиниция на $\gamma_i^k$ и разменяне на суми
\end{itemize}

\subsection{Основна теорема}
За $X$ - възвратна и неразложима МВ имаме:
\begin{enumerate}
\item $\gamma_k^k = 1$
\item $\gamma^k = \gamma^k P$ - стационарна мярка
\item $\gamma_i^k > 0$ и $\gamma_i^k < \infty$ за всяко $i \in I$
\item Ако $\lambda$ е стационарна мярка и $\lambda_k = 1$, то $\lambda = \gamma^k$
\item[4.1] Ако МВ е само неразложима, то ако $\lambda$ е стационарна мярка и $\lambda_k = 1$, то $\lambda \geq \gamma^k$ и може $\gamma^k$ да не е стационарна мярка
\end{enumerate}

\subsubsection*{Доказателство:}
\begin{enumerate}
\item[(1)] Очевидно
\item[(2)] $\gamma_i^k$ го написваме като сума от $n=1$ до $\infty$ от $P_k(X_n = i; n \leq T_1^k)$. Разбиваме го по всички $X_{n-1}$, \\
взимаме условна вероятност, прилагаме СМС, пренареждаме сума и получаваме $\gamma_i^k = \sum_{j \in I} p_{ji}\gamma_j^k$. Готово.
\item[(3)] 
\begin{itemize}
\item За $\gamma_i^k > 0$: От неразложимост $p_{ki}^{(l_1)} > 0$ за някое $l_1$. От $\gamma^k = \gamma^k P^{l_1}$ имаме \\
$\gamma_i^k = \sum_j p_{ji}^{(l_1)}\gamma_j^k \geq p_{ki}^{(l_1)}\gamma_k^k = p_{ki}^{(l_1)} > 0$
\item За $\gamma_i^k < \infty$: От неразложимост $p_{ik}^{(l_2)} > 0$ за някое $l_2$. От $\gamma^k = \gamma^k P^{l_2}$ и $\gamma_k^k = 1$ имаме \\
$1 = \gamma_k^k = \sum_j p_{jk}^{(l_2)}\gamma_j^k \geq p_{ik}^{(l_2)}\gamma_i^k$. Делим двете страни на $p_{ik}^{(l_2)}$ и получаваме $\gamma_i^k \leq 1/p_{ik}^{(l_2)} < \infty$
\end{itemize}
\item[(4.1)] Нека МВ е само неразложима. Нека $\lambda$ е стационарна мярка и $\lambda_k = 1$. Тогава $1 = \lambda_k \geq \gamma_k^k = 1$. Нека $i \neq k$. \\
Тогава $\lambda_i = \sum_{j \in I} \lambda_j p_{ji} = \lambda_k p_{ki} + \sum_{j \neq k} \lambda_j p_{ji} = P_k(X_1 = i, T_1^k \geq 1) + \sum_{j \neq k} \lambda_j p_{ji}$ \\
От сумата по подобен начин вадим $P_k(X_2 = i; T_1^k \geq 2)$ и продължаваме рекурсивно. Ако продължим $m$ пъти и изрежем остатъчната сума \\
получаваме неравенството $\lambda_i \geq P_k(X_1 = i; T_1^k \geq 1) + P_k(X_2 = i; T_1^k \geq 2) + \dots + P_k(X_m = i; T_1^k \geq m)$. Но $m$ е произволно, тоест \\
можем да пуснем границата и накрая получаваме $\lambda_i \geq P_k(X_1 = i; T_1^k \geq 1) + P_k(X_2 = i; T_1^k \geq 2) + \dots = \gamma_i^k$. Готово.
\item[(4)] Нека МВ е възвратна и неразложима. Тогава $\gamma^k$ е стационарна мярка. Нека $\lambda$ е друга такава с $\lambda_k = 1$. Ще видим че $\lambda = \gamma^k$. \\
Разглеждаме $\mu = \lambda - \gamma^k \geq 0$. $\mu$ е стационарна мярка, защото $\mu = \lambda - \gamma^k = \lambda P - \gamma^k P = (\lambda - \gamma^k)P = \mu P$. Допускаме, че съществува $i \in I: \mu_i > 0$. \\
Знаем че $\mu_k = 0$, защото $\lambda_k = \gamma_k^k = 1$. От неразложимост съществува $l: p_{ik}^{(l)} > 0$. $0 = \mu_k = \sum_{j \in I} \mu_j p_{jk}^{(l)} \geq \mu_i p_{ik}^{(l)} > 0$. Противоречие. \\
Това значи, че $\lambda = \gamma^k$.
\end{enumerate}

\subsection{Предположения и следствия}
\subsubsection*{Предположение:}
От тази теорема изглежда все едно веригата има доста стационарни мерки, но всъщност 
те са една и съща с точност до умножение по скалар - за различни $k$ се умножава по конкретно число, 
което прави $\gamma_k^k = 1$.

\subsubsection*{Наблюдение:}
Ако $\lambda$ е стационарна мярка, то $c\lambda$ отново е стационарна мярка, защото 
$c\lambda = c(\lambda P) = (c\lambda)P$ - мярките винаги са безкрайно много.

\subsubsection*{Следствие:}
\begin{itemize}
\item Всяка неразложима и възвратна МВ има стационарна мярка - именно $\gamma^k$
\item Всяка неразложима и крайна МВ има стационарен вектор - $\gamma^k/m_k$ тъй като $m_k$ е крайна сума, т.е. е $< \infty$
\end{itemize}

\subsubsection*{Забележка:}
Ако МВ има стационарна мярка $\gamma^k$ и $m_k < \infty$, то тя има и стационарен вектор.

\subsection{Дефиниции за възвратност}
\subsubsection*{Дефиниция:}
Възвратно състояние $k \in I$ се нарича:
\begin{itemize}
\item \textbf{положително възвратно}, ако $m_k < \infty$
\item \textbf{нулево възвратно}, ако $m_k = \infty$
\end{itemize}

\subsection{Теорема за положителна възвратност}
Нека $X$ е неразложима МВ. Тогава следните са еквивалентни:
\begin{enumerate}
\item всяко състояние е положително възвратно
\item поне едно състояние е положително възвратно
\item МВ има стационарен вектор
\end{enumerate}

\subsubsection*{Забележка:}
Ако $X$ е неразложима положително възвратна МВ, то $(\pi_i)_{i \in I}$ се задават чрез $\pi_i = 1/m_i \in (0; \infty)$ и този стационарен вектор е единствен.

\subsubsection*{Доказателво:}
\begin{itemize}
\item $(1) \Rightarrow (2)$ - тривиално
\item $(2) \Rightarrow (3)$: \\
Нека $k$ е положително възвратно - $m_k < \infty$. Щом $k$ е възвратно, то цялата МВ е възвратна. Тогава $\gamma^k$ е стационарна мярка и $\pi = \gamma^k/m_k$ е стационарен вектор.
\item $(3) \Rightarrow (1)$: \\
Идеята е следната: Понеже $\pi$ се сумира до едно, то има такова $j$, за което $\pi_j > 0$. От неразложимост и $\pi_j > 0$ намираме, че $\pi_i > 0$ $\forall i \in I$. \\
$\lambda = \pi/\pi_k$ е стационарна мярка с $\lambda_k = 1$. Но щом $\lambda$ е стационарна мярка, то от горната теорема $\lambda \geq \gamma^k$. От това $\sum \lambda_j \geq \sum \gamma_j^k$. \\
Но $\sum \lambda_j = \sum \pi_j/\pi_k = 1/\pi_k < \infty$, тоест $\sum \gamma_j^k = m_k < \infty$. Това ни казва че очакваното време за завръщане в $k$ от $k$ е крайно. \\
Това значи че $k$ реално е възвратно по дефиниция - винаги ще се върнем, очакването е крайно. Тоест $k$ е възвратно. Но $m_k < \infty$. \\
Тогава $k$ е положително възвратно. Но ние избрахме $k$ произволно, тоест всяко състояние е положително възвратно.
\end{itemize}

\subsubsection*{Забележка:}
От тази теорема, щом МВ има стационарен вектор, то тя е положително възвратна. Но от предишната теорема става 
ясно че $\lambda = \gamma^k = \pi/\pi_k$. Тогава щом $\sum \lambda_i = \sum \pi_i/\pi_k = 1/\pi_k$, то $m_k = \sum \gamma_i^k = \sum \lambda_i = 1/\pi_k$ или $m_k = 1/\pi_k$. 
- Тоест за една неразложима и положително възвратна МВ имаме, че $m_k = 1/\pi_k$ и $\gamma^k/m_k = \pi$, тоест $\gamma_i^k/m_k = \pi_i$ за всяко $i$, и като заменим $\pi_i = 1/m_i$, то излиза $\gamma_i^k = m_k/m_i$.

\subsubsection*{Забележка:}
Тук се говори същото като мярката за важност, която я говорихме преди. Там $v_i = 1/E_i[U_1]$, което реално е същото като $\pi_i = 1/m_i$.

\section{Сходимост към стационарност и апериодичност}

\subsection{Основна теорема за сходимост}
Нека $X$ е неразложима и стационарна (съществува стационарен вектор) МВ. Тогава $\forall i,j \in I$:
\begin{enumerate}
\item $P_j\left(\lim_{N\to\infty} \frac{V_i(N)}{N} = \pi_i\right) = 1$, където $V_i(N)$ е броя посещения на $i$ до момента $N$.
\item $\lim_{N\to\infty} \frac{1}{N}\sum_{n=0}^N P_j(X_n = i) = \pi_i$ - Средната вероятност да сме в $i$ клони към $\pi_i$
\item $P_\lambda\left(\lim_{N\to\infty} \frac{V_i(N)}{N} = \pi_i\right) = 1$ - Логично, тъй като с времето има все по-малко значение откъде почваме.
\end{enumerate}

\subsubsection*{Доказателство:}
\begin{enumerate}
\item[(1)] Разглеждали сме, че $v_i = \lim_{N\to\infty} \frac{\text{\# посещения на }i\text{ до момента }N}{N} = \pi_i$. С други думи $P_i(v_i = \pi_i) = 1$. \\
Трябва да видим, че $P_j(v_i = \pi_i) = 1$. Щом съществува стационарен вектор, значи веригата е положително възвратна по теорема от миналата лекция. \\
Тогава $P_j(H^i < \infty) = 1$, и тогава:

\begin{align*}
P_j(v_i = \pi_i) &= P_j(v_i = \pi_i; H^i < \infty) \\
&= P_j\left(\lim_{N\to\infty} \frac{\text{\# посещения на }i\text{ от момента }H^i\text{ до момента }N}{N} = \pi_i \mid H^i < \infty\right) \\
&\overset{СМС}= P_i\left(\lim_{N\to\infty} \frac{\text{\# посещения на }i\text{ от момента }0\text{ до момента }N-H^i}{N} = \pi_i\right) \\ 
&=P_i(v_i=\pi_i)=1
\end{align*}

което следва от СМС - взимаме $Y_n$ която започва от $X_{H^i}$. \\
И после излиза от факта, че $P_i(v_i = \pi_i) = 1$, което го знаем.

\item[(2)] От (1) имаме че $\lim_{N\to\infty} \frac{1}{N}\sum_{n=0}^N 1_{\{X_n = i\}} = \pi_i$ независимо откъде тръгваме. \\
Тогава:
\begin{align*}
\pi_i &= E_j[\pi_i] = E_j\left[\lim_{N\to\infty} \frac{1}{N}\sum_{n=0}^N 1_{\{X_n = i\}}\right] \\
&= \lim_{N\to\infty} \frac{1}{N}\sum_{n=0}^N E_j[1_{\{X_n = i\}}] = \lim_{N\to\infty} \frac{1}{N}\sum_{n=0}^N P_j(X_n = i)
\end{align*}
Очакването и границата може да се разменят само когато това в границата е ограничено отгоре от някоя константа. \\
От това твърдение става ясно, че $\pi_i$ може да се намери като сходимост по Чезаро от $p_{ji}^{(n)}$.

\item[(3)] Няма да доказваме.
\end{enumerate}

\subsection{Следствие (ергодичност)}
$\lim_{N\to\infty} \frac{1}{N}\sum_{k=0}^N f(X_k) = E_\pi[f(X_1)] = \sum_{i \in I} \pi_i f(i)$

За Бонус-Малус искаме дългосрочния среден коефициент да е 1. $E_\pi[f(X_1)]$ може да се сметне като число. 
Ние моделирайки коефициентите един вид избираме функцията $f$, и след като сме избрали трябва да проверим 
стойността на $E_\pi[f(X_1)]$, която от това следствие ни казва дългосрочния среден коефициент.

\subsection{Размишления за сходимост}
От доказателството на (2) от теоремата горе виждаме че чрез сходимост по Чезаро (сходимост 
по средно аритметично) може да намерим стационарен вектор. Обаче чрез нормалната сходимост това невинаги е възможно, 
защото границата може да не съществува. А нормалната сходимост е по-лесна за изчисление, например когато 
диагонализираме матрицата на преход или когато умножаваме $P, P^2, P^4, P^8,...$ за да намерим приближение на $\lim_{n\to\infty}P^n$ използваме имплицитно факта че границата съществува. Интересуваме се кога точно може да разчитаме, че тази граница съществува и кога може да бъде използвана за намиране на стационарен вектор.

\subsection{Дефиниции за апериодичност}
\subsubsection*{Дефиниция 1:}
Състояние $i \in I$ е \textbf{апериодично} ако за някое $n_i$, $p_{ii}^{(n)} > 0$ за всяко $n \geq n_i$.

\subsubsection*{Дефиниция 2 (еквивалентна):}
Състояние $i \in I$ е \textbf{апериодично} ако $\text{НОД}\{n \geq 1 : p_{ii}^{(n)} > 0\} = 1$.

\subsubsection*{Следствие:}
Ако $p_{ii} > 0$, то $i$ е апериодично.

\subsection{Теорема за апериодичност}
Нека $X$ е неразложима МВ и $i \in I$ е апериодично. Тогава за всеки две състояния от веригата и за достатъчно 
голямо $n_0$ е вярно че има път с точно $n$ стъпки от едното до другото състояния за всяко $n \geq n_0$. 
Двете състояния може да съвпадат, което ги прави и апериодични. Но те са произволни, тоест всяко състояние е апериодично.

\subsubsection*{Доказателво:}
От неразложимост има път $j_1 \to i$ и $i \to j_2$. Нека $p_{j_1i}^{(l_1)} > 0$ и $p_{ij_2}^{(l_2)} > 0$. Също знаем, че $i$ е апериодично, тоест 
има $l_3$, за което всяко $l > l_3$: $p_{ii}^{(l)} > 0$. Тогава е ясно че за всяко $l \geq l_1 + l_2 + l_3$ има път $j_1 \to j_2$ за $l$ стъпки - за всяка 
нова стъпка правим допълнителен преход $i \to i$ в средата. Формално се прави с теоремата на Чапман-Колмогоров.

\subsection{Теорема за сходимост при апериодичност}
За $X$ неразложима, апериодична и стационарна МВ е вярно че $\lim_{n\to\infty} P_\lambda(X_n = i) = \pi_i$.

\subsubsection*{Размишление:}
Тоест горното размишление е вярно само за апериодични МВ. Само за апериодичните МВ 
можем да намерим стационарен вектор чрез нормална сходимост, за периодичните не е възможно с нормална сходимост, 
там вече можем да използваме сходимост по Чезаро.

\subsubsection*{Доказателство (идея):}
Нека $Y$ е същата като $X$, но започваща от $\pi$ вместо $\lambda$. И двете $X$ и $Y$ са неразложими и стационарни МВ, тоест има момент $T$, в 
който $X_T = Y_T = b \in I$ и този момент е краен с вероятност 1. Тези факти трябва да се съобразят по-формално ама за това да се 
видят записките. Взимаме нова МВ $Z$, която я дефинираме като $Z_n = X_n$ за $n \leq T$ и $Z_n = Y_n$ за $n > T$. Първо, $Z_n = X_n$ по разпределение 
за всяко $n$ по-малко от $T$ по дефиниция. Тъй като $Z$ е дефинирано като $Y_n$ за $n$ по-голямо от $T$, то трябва да докажем 
че $X_n = Y_n$ по разпределение за $n > T$. Не знам на лекции защо толкова много е писано, обаче на мен ми се струва очевидно по СМС - все пак 
в момента $T$, $X$ и $Y$ съвпадат, а след това имат еднакви матрици на прехода и следователно и вероятностите им са същите. 
Тоест $Z_n = Y_n = X_n$ по разпределение за $n \geq T$. Тоест $Z_n = X_n$ по разпределение за всяко $n$. За да видим, че 
$X_n$ схожда към стационарния вектор, то гледаме грешката $|P_\lambda(X_n = i) - \pi_i|$. Ще видим че за големи $n$ схожда към 0. 
$|P_\lambda(X_n = i) - \pi_i| = |P(Z_n = i) - P(Y_n = i)|$, тъй като $X_n = Z_n$ по разпределение, а $Y$ е в стационарност още от началото. Разбиваме по случаи 
спрямо $n < T$ или $n \geq T$, джуркаме там нещо, и накрая излиза, че грешката $\leq 2P(T > n)$, което пък схожда до 0.

\subsection{Следствие за крайни апериодични МВ}
Нека $X$ е апериодична, неразложима и крайна МВ. Тогава съществуват $N \geq 1$, $\epsilon > 0$, $q := 1 - \epsilon|I| \geq 0$ такива че:
$$\sum_{i \in I} |\pi_i - p_{ji}^{(n)}| \leq 2q^r, \quad \text{където } n = rN + l$$
В допълнение е вярно и за всяко индивидуално $i,j$ вместо цялата сума.

\subsubsection*{Доказателво:}
Дефинираме $N = \min\{n\in \mathbb{N} : p_{j_1j_2}^{(n)} > 0 \ \forall j_1,j_2 \in I\}$. $N < \infty$, защото веригата е апериодична. Дефинираме $\epsilon = \min_{j_1,j_2 \in I} p_{j_1j_2}^{(N)} > 0$. 
Всеки елемент на матрицата $P^N = Q$ може да се запише като $q_{ij} = \epsilon + \tilde{q}_{ij}$, $\tilde{q}_{ij} \geq 0$. 
Тогава за произволно $i$: $q := \sum_{j=1}^k \tilde{q}_{ij} < 1 = \sum_{j=1}^k q_{ij}$. 
Също така $q \geq 0$, защото $\tilde{q}_{ij} \geq 0$ за всяко $j$. Тоест $0 \leq q < 1$. \\

Сега ще видим, че $||\mu Q - \nu Q|| \leq q||\mu - \nu||$ за всеки две разпределения $\mu$ и $\nu$. 
$||\mu Q - \nu Q|| = \sum_{i=1}^k |(\mu Q)_i - (\nu Q)_i|$. 
Но $(\mu Q)_i = \sum_{j=1}^k \mu_j (\epsilon + \tilde{q}_{ij})$. $(\nu Q)_i$ - аналогично. Тогава заменяме тези 
представяния в $|(\mu Q)_i - (\nu Q)_i|$. Групираме нещата с $\epsilon$ заедно и нещата без $\epsilon$ заедно. 
Вкарваме модула в сумата чрез неравенството на триъгълника. Неравенството се запазва като махнем нещата с $\epsilon$, 
тъй като те са в модул, тоест са положителни. Накрая получаваме $q||\mu - \nu||$, тоест $||\mu Q - \nu Q|| \leq q||\mu - \nu||$. \\

Сега това твърдение ще използваме за разпределенията $\pi$ и $\delta_j$ - те съответстват на търсената в условието разлика. \\
Тогава за $n \geq N$:
\begin{align*}
||\pi P^n - \delta_j P^n|| &= ||\pi P^{kN + l} - \delta_j P^{kN + l}|| \\
&= ||\pi P^l Q^k - \delta_j P^l Q^k|| \\
&\leq q^k ||\pi P^l - \delta_j P^l|| \\
&\leq q^k (||\pi P^l|| + ||\delta_j P^l||) \\
&\leq q^k (1 + 1) = 2q^k
\end{align*}
$||\pi P^l|| = ||\delta_j P^l|| = 1$, защото всяко от тях си е просто едно разпределение (за $X_l = i$). \\

Доказахме, че $||\pi P^n - \delta_j P^n|| \leq 2q^k$. Но $||\pi P^n - \delta_j P^n||$ е точно $\sum_{i \in I} |\pi_i - p_{ji}^{(n)}|$ по дефиницията на мярката.

\section{Обратимост на Марковски вериги}
Интересуваме се кога за $X \sim Markov(\lambda,P)$ е вярно, че за всяко $M\geq1: (X_{M-n})_{0\leq n \leq M} \sim Markov(\lambda,P)$.
\subsection{Теорема за обратимост}
Нека $X$ е неразложима МВ със стационарен вектор $\pi$ и матрица $P$. Ако $X_0 \sim \pi$, то $(X_{M-n})_{0 \leq n \leq M}$ са първите $M$ стъпки на неразложима МВ със стационарен вектор $\pi$ и матрица $\hat{P}$, където $\hat{p}_{ij} = p_{ji}\frac{\pi_j}{\pi_i}$.

\subsubsection*{Доказателство:}
\begin{enumerate}
\item Проверяваме че $\hat{P}$ е матрица на прехода:
$$\sum_j \hat{p}_{ij} = \sum_j p_{ji}\frac{\pi_j}{\pi_i} = \frac{1}{\pi_i}\sum_j \pi_j p_{ji} = \frac{\pi_i}{\pi_i} = 1$$

\item Проверяваме вероятностите за траектория на $Y_n = X_{M-n}$:
\begin{align*}
P(Y_0=i_0, \dots, Y_M=i_M) &= P(X_M=i_0, \dots, X_0=i_M) \\
&= \pi_{i_M} p_{i_M i_{M-1}} \cdots p_{i_1 i_0} \\
&= \hat{p}_{i_{M-1} i_M} \pi_{i_{M-1}} p_{i_{M-1} i_{M-2}} \cdots p_{i_1 i_0} \\
&= \dots = \pi_{i_0} \hat{p}_{i_{M-1} i_M} \cdots \hat{p}_{i_0 i_1}
\end{align*}
Тоест $Y \sim \text{Markov}(\hat{P}, \pi)$ по теоремата за характеризация.

\item Проверяваме стационарност на $\pi$ за $Y$:
$$(\pi \hat{P})_i = \sum_j \pi_j \hat{p}_{ji} = \sum_j \pi_j p_{ij} \frac{\pi_i}{\pi_j} = \pi_i \sum_j p_{ij} = \pi_i$$

\item Проверяваме неразложимост на $Y$:
С индукция по $n$ доказваме $\hat{p}_{ji}^{(n)} = p_{ij}^{(n)} \frac{\pi_i}{\pi_j}$. От неразложимост на $X$, $\exists n$ с $p_{ij}^{(n)} > 0$, следователно $\hat{p}_{ji}^{(n)} > 0$.
\end{enumerate}

\subsection{Дефиниция за обратимост}
$X \sim \text{Markov}(\lambda,P)$ е \textbf{обратима} при започване от $\lambda$, ако:
$$\forall M \geq 1\ \forall i_0,\dots,i_M \in I$$
$$P_\lambda(X_0=i_0, \dots, X_M=i_M) = P_\lambda(X_M=i_0, \dots, X_0=i_M)$$
или еквивалентно, $(X_{M-n})_{n \leq M}$ има същото разпределение като първите $M$ стъпки на $X$.

\subsection{Теорема (Условие на баланс)}
Една неразложима $X \sim \text{Markov}(\lambda,P)$ е обратима $\Leftrightarrow$ $\forall i,j \in I$ $\lambda_i p_{ij} = \lambda_j p_{ji}$ $\Rightarrow$ $\lambda$ е стационарен вектор.

\subsubsection*{Забележки:}
\begin{enumerate}
\item За обратими МВ $\hat{P} = P$ (следва от УБ и първата теорема)
\item УБ не означава $P = P^T$, но ако $P = P^T$ и $\pi$ е равномерно, то веригата е обратима
\item Всяка обратима МВ има стационарен вектор
\end{enumerate}

\subsubsection*{Доказателво:}
\begin{itemize}
\item[$\Leftarrow$)] За произволна траектория:
\begin{align*}
P(X_0=i_0, \dots, X_M=i_M) &= \lambda_{i_0} p_{i_0 i_1} \cdots p_{i_{M-1} i_M} \\
&= p_{i_1 i_0} \lambda_{i_1} p_{i_1 i_2} \cdots p_{i_{M-1} i_M} \quad \text{(по УБ)} \\
&= \dots = \lambda_{i_M} p_{i_M i_{M-1}} \cdots p_{i_1 i_0} \\
&= P(X_M=i_0, \dots, X_0=i_M)
\end{align*}

\item[$\Rightarrow$)] За $M=1$:
$$P(X_0=i, X_1=j) = P(X_1=i, X_0=j) \Rightarrow \lambda_i p_{ij} = \lambda_j p_{ji}$$

Доказваме $\lambda$ е стационарен:
$$(\lambda P)_i = \sum_j \lambda_j p_{ji} = \sum_j \lambda_i p_{ij} = \lambda_i \sum_j p_{ij} = \lambda_i$$
\end{itemize}

\section{Бонус материал за бонус малус}
\subsection{Означения}
$\langle x,y \rangle$ - скаларно произв.

\subsection{Свойства}
За всяка матрица е вярно, че $\langle Ax,y \rangle = \langle x,A^t y \rangle$. В частност, ако $A$ е симетрична, тоест $A=A^t$, то $\langle Ax,y \rangle = \langle x,Ay \rangle$.

По-общо, не е нужно $P$ да е симетрична, но поне трябва да е изпълнено условието за баланс за да е вярно, че $\langle Px,y \rangle = \langle x,Py \rangle$.

Горното твърдение важи и за претеглено скаларно произведение, тоест за всяко разпределение $\lambda$ и за $P$ удовлетворяващо условието на баланса, то имаме че $\langle Px,y \rangle_\lambda = \langle x,Py \rangle_\lambda$, където $\langle x,y \rangle_\lambda := \sum_{i} x_i y_i \lambda_i$. Доказва се по дефиниция на претеглено средно и условие на баланс.

\subsection{Приложение за Марковски вериги}
Нека $x := (f(1),\dots,f(N))$ е вектор, съпоставящ на всяко състояние число.

$Px = \left( \sum_{j=1}^N p_{ij} x_j \right)_i = \left( \sum p_{ij} f(j) \right)_i = \left( E_i[f(X_1)] \right)_i$.

Аналогично $P^n x = \left( E_i[f(X_n)] \right)_i$.

\subsection{Спектрални свойства}
Сега следват някакви неща от линейна алгебра, които не знам как се доказват:

За неразложима, обратима, апериодична МВ е вярно, че:
\begin{itemize}
\item $\mu_1=1$ е собствена стойност, която е най-голямата по модул (и единствена с кратност 1)
\item Всички други собствени стойности са $|\mu_j|<1$ за $j\geq2$
\item Матрицата $P$ за такива вериги има ортонормиран базис $(v_i)_{1\leq i\leq N}$
\end{itemize}

Ортонормиран ще рече, че $\langle v_i,v_j \rangle_\lambda = \delta_{ij}$. Тъй като $(v_i)$ е базис, то всеки вектор $x$ може да се представи като $x = \sum_{j=1}^N c_j v_j$, където $c_j = \langle x,v_j \rangle_\lambda$. Не знам защо $c_j$ е това.

\subsection{Развитие на процеса}
Това $x$ го заместваме в $P^n x$ и се получава:
$$P^n x = \sum_{j=1}^N \langle x,v_j \rangle_\lambda P^n v_j$$

Обаче $v_j$ е собствен вектор със собствена стойност $\mu_j$, тоест $P^n v_j = \mu_j^n v_j$. От сумата изкраваме члена за $j=1$, а другото остава. Този член е $\langle x,v_1 \rangle_\lambda v_1$.

Обаче $v_1$ е точно собственият вектор на $\mu_1=1$, тоест $P v_1 = v_1$. Това е изпълнено при $v_1 = (1)_{1\leq i\leq N}$, защото $P$ е стохастична матрица.

С други думи:
$$\langle x,v_1 \rangle_\lambda v_1 = \langle x,(1) \rangle_\lambda (1)$$

Останалата част на сумата е $O(|\mu_2|^n)$, защото $|\mu_2| \geq |\mu_j|$ за $j\geq2$.

Но $\langle x,(1) \rangle_\lambda (1) = E_\lambda[f(X_1)]$ чрез малко преработка. Но $P^n x = (E_i[f(X_n)])_i$ или $P^n x$ е вектор от средният бонус малус коефициент в година $n$ спрямо началното състояние $i$.

\subsection{Интерпретация за бонус малус}
Би трябвало за големи $n$ всички стойности в този вектор да клонят към 1 за да е честен бонус малус коефициента. Когато го заместим в равенството получаваме, че:
$$|E_i[f(X_n)] - E_\lambda[f(X_1)]| = O(|\mu_2|^n)$$

тоест веригата клони към стационарност експоненциално спрямо $\mu_2$.

Когато веригата не е обратима (в случая на бонус малус), то равенството е:
$$|E_i[f(X_n)] - E_\lambda[f(X_1)]| = O(n^{k_2-1}|\mu_2|^n)$$

където $k_2$ е кратността на $\mu_2$ като собствена стойност.

За бонус малус ще трябва да сметнем $E_\lambda[f(X_1)]$, както и $k_2$ и $\mu_2$, с компютър. Искаме да намерим за колко години средният бонус малус се отклонява от 1 с някаква (фиксирана) грешка.

\section{Поасонови процеси}

\subsection{Означения}
$\mu_A: \Omega \to \mathbb{N} \cup \{\infty\}$ - Брой точки в $A$ \\
$\nu(A) = E[\mu_A]$ - Интензитет

\subsection{Дефиниции}
\subsubsection*{Независимост}
За непресичащи се $A_j$, $\mu_{A_j}$ са независими

\subsubsection*{Случайна точкова мярка}
Множество от точки в $U$.

\subsubsection*{Поасонова случайна мярка}
Случайна точкова мярка, която притежава свойството независимост и $\mu_A \sim \text{Poi}(\nu(A))$.

\subsubsection*{Поасонова брояща мярка}
Поасонова случайна мярка, за която $U = \mathbb{R}_+ \times \{1\}$

\subsubsection*{Поасонов броящ процес}
$N=(N_t)_{t\geq0}$, $N_0=0$ се нарича \textbf{Поасонов броящ процес} с интензитет $\lambda>0$ \\
ако $N$ има независими и стационарни нараствания и $\mu_{[0,t]}=N_t\sim\text{Poi}(\lambda t)$, $t\geq0$; $E[\mu_{[a,b]}]=\nu((a,b))=\lambda(b-a)$

\subsubsection*{Съставен Поасонов процес}
Нека $N=(N_t)_{t\geq0}$ е броящ Поасонов процес. Нека $(\xi_n)_{n\geq1}$ са i.i.d. и независими от $N$. \\
Тогава $Y_t=\sum_{n=1}^{N_t} \xi_n$ се нарича \textbf{Съставен Поасонов Процес}.

\end{document}
